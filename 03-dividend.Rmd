# Exercise 3: Dividend data

In this example, our goal is to develop a neural network to determine if a stock pays a dividend or not. The dataset is stored under `dividendinfo.csv`, which includes one response variable and five predictor variables.

* `dividend` (class): A value of 1 indicates that the stock pays a dividend; 0 indicates that the stock that does not pay a dividend. 
* `fcfps`: Free cash flow per share (in $)
* `earnings_growth`: Earnings growth in the past year (in %)
* `de`: Debt to Equity ratio
* `mcap`: Market Capitalization of the stock
* `current_ratio`: Current Ratio (or Current Assets/Current Liabilities)

**Task**

1. Read in the data and perform exploratory analysis. What have you observed?

```{r echo=FALSE}
dividend <- read.csv("dividendinfo.csv")
```


```{r eval=FALSE, webex.hide="Solution"}
dividend <- read.csv("dividendinfo.csv")

# some example codes for numerical summaries
summary(dividend)
library(skimr)
skim(dividend)

# some example codes for graphical summaries
pairs(dividend)
libray(GGally)
ggpairs(dividend)
par(mfrow=c(3,2));
invisible(lapply(2:ncol(dividend),function(i) boxplot(dividend[,i]~dividend$dividend)))
```

2. Pre-process and split the data to prepare for training and evaluating a neural network. 

`r hide("Hint")` 
As all variables are continuous and they have quite different ranges, scale them either using standardisation or min-max normalisation. 
`r unhide()`

`r hide("Solution")` 
While there are built-in functions such as `scale` to standardise the entire data, the best practice is to split the data into training and test first and then apply standardisation/normalisation. This could avoid information leakage from training to test data. 
```{r}
# Data split
set.seed(1)
idx <- sample(nrow(dividend),0.8*nrow(dividend))
train <- dividend[idx,]
test <- dividend[-idx,]

# option 1: Standardise the data
standardisation <- function(x){
  (x - mean(x)) / sd(x)
}
train.std <- apply(train[,2:6], 2, standardisation)
train.std <- cbind(train[,1],train.std)
test.std  <- apply(test[,2:6], 2, standardisation)
test.std  <- cbind(test[,1],test.std)

# option 2: Normalise the data
min_max_scale <- function(x){
  (x - min(x)) / (max(x) - min(x))
}
train.norm <-apply(train[,2:6], 2, min_max_scale)
train.norm <-cbind(train[,1],train.norm)
test.norm  <-apply(test[,2:6],  2, min_max_scale)
test.norm  <-cbind(test[,1], test.norm)
```
`r unhide()`

3. Build a neural network with a single hidden layer, any number of hidden nodes, and the logistic function as the activation function. Interpret the relative importance of variables using the `garson` function. 
`r hide("Solution")` 
```{r}
set.seed(1)
nn_di <- neuralnet(dividend~., data=train, hidden=c(5), err.fct="ce", act.fct="logistic", linear.output=FALSE, likelihood=TRUE)

garson(nn_di)
```
We can see that the variable `current_ratio` is the one with the strongest relationship with the response variable `dividend`, followed by `de`, `fcfps` and `mcap`. The variable `earnings_growth` has the least relationship with `dividend`.
`r unhide()`

4. Fit the above model multiple times using the argument `rep` and select the optimal model. 

`r hide("Solution")` 
```{r}
set.seed(1)
nn_di <- neuralnet(dividend~., data=train, hidden=c(5), err.fct="ce", act.fct="logistic", linear.output=FALSE, likelihood=TRUE, rep=5)

# plot(nn_di)
plot(nn_di, rep="best")
```
From the plots (`plot(nn_di)`), we can see that the optimisation algorithm stops at different iterations (from `Steps`) and lead to differnt cross-entropy loss (from `Error`). In general, training the network longer decreases the cross-entropy loss. However, this decrease takes place on the training data set and may not generalise to the test. In other words, training the model longer may increase the risk of overfitting. 

To select the optimal model, we could look at the AIC and BIC values.
```{r}
which.min(nn_di$result.matrix[4,]) #AIC
which.min(nn_di$result.matrix[5,]) #AIC
```
AIC and BIC agree in this case and they both choose the third repetition. 
`r unhide()`


